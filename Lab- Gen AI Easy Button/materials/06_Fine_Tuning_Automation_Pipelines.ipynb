{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafc3aec-335c-4191-b849-ceea91a33830",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0237c-5892-448e-85c1-7722e12ac717",
   "metadata": {},
   "source": [
    "# 6.0 Automation Workflows/Pipelines of Fine-tuning \n",
    "\n",
    "The diagram presented provides a high-level overview of a fine-tuning workflow, showcasing multiple interconnected components. Each component in the diagram corresponds to a [Cluster Workflow Templates](https://argo-workflows.readthedocs.io/en/latest/cluster-workflow-templates/) within Argo Workflows. These individual components are combined to form an overarching  [Workflow template](https://argo-workflows.readthedocs.io/en/latest/workflow-templates/), ensuring a modular and reusable structure. By leveraging Argo Workflows, the fine-tuning process is automated, enhancing efficiency and scalability.\n",
    "\n",
    "\n",
    "<center><img src=\"./images-dli/example-workflow.png\" style=\"width: 800px;\"></center>\n",
    "\n",
    "Following steps are involved in the workflow: \n",
    "\n",
    "1. Create dataset in Nemo Datastore.\n",
    "2. Upload files in the created dataset.\n",
    "3. Register dataset in Nemo Entity Store.\n",
    "4. Create customization job within Nemo Customizer.\n",
    "5. Track the cusomization job status until its completed.\n",
    "6. Then create evaluation target and configuration, and create evaluation on the fine-tuned model. \n",
    "\n",
    "\n",
    "This lab builds upon the automation steps implemented in the previous lab using Argo Workflows. Previously, steps required manual intervention, but in this iteration, those manual steps have been encapsulated within Cluster Workflow Templates. As a result, the entire fine-tuning workflow is now fully automated within Argo, reducing human intervention and ensuring consistency in execution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5ea54-a01a-4ea3-bbee-9b405006eec6",
   "metadata": {},
   "source": [
    "## 6.0 Local Lab's Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e850b1-1575-4c55-b0c7-b29545c320e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Add the git Username\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382ec47-9648-4a59-a3f6-8d8a825ca0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please enter your Git username:\")\n",
    "git_username = input()\n",
    "\n",
    "# Add assertions to validate the variables\n",
    "assert git_username and git_username.strip(), \"GitHub username cannot be empty\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bac37-c7db-4300-8a3e-7c3e43c64706",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_name=\"llmops-nvidia\"\n",
    "git_base_url=\"github.com\"\n",
    "applications_base_dir = \"llmops-nvidia/applications\"\n",
    "secrets_base_dir = \"llmops-nvidia/secrets\"\n",
    "ingress_base_dir = \"llmops-nvidia/ingress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e238dc-462a-4416-bc6f-5377f8ca7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_url=f\"git@{git_base_url}:{git_username}/{git_repo_name}.git\" \n",
    "git_repo_url_ssh=f\"ssh://git@{git_base_url}/{git_username}/{git_repo_name}.git\"\n",
    "commit_name=git_username \n",
    "commit_email=f\"{git_username}@llmops-nvidia\"\n",
    "print(git_repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb9387-51df-41f7-b457-1b95db75b003",
   "metadata": {},
   "source": [
    "### 6.0.1 Add datasets to MinIO \n",
    "\n",
    "We add datasets to the local minio, so that we can use in the automation. We will refer the bucket from the automation pipeline. \n",
    "\n",
    "Output: \n",
    "\n",
    "```\n",
    "Bucket 'test-dataset' created.\n",
    "Uploaded: /dli/task/dataset/validation/validation.jsonl -> validation/validation.jsonl\n",
    "Uploaded: /dli/task/dataset/training/training.jsonl -> training/training.jsonl\n",
    "Uploaded: /dli/task/dataset/testing/testing.jsonl -> testing/testing.jsonl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18deeccf-8399-436c-b489-cf42b363eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "# MinIO Configuration\n",
    "minio_url = \"minio.local\"  \n",
    "access_key = \"admin\"\n",
    "secret_key = \"QuX2+P16SPc7\"\n",
    "bucket_name = \"test-dataset\"\n",
    "local_base_path = \"/dli/task/dataset\"  # Local base directory containing the folders\n",
    "\n",
    "# Initialize MinIO Client\n",
    "client = Minio(\n",
    "    minio_url,\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key,\n",
    "    secure=False,  # Set to True if using HTTPS\n",
    ")\n",
    "\n",
    "# Ensure the bucket exists\n",
    "try:\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' created.\")\n",
    "except S3Error as e:\n",
    "    print(f\"Error checking/creating bucket: {e}\")\n",
    "\n",
    "# Upload all files from the base directory and subfolders\n",
    "for root, _, files in os.walk(local_base_path):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root, file)  # Full local path\n",
    "        relative_path = os.path.relpath(local_file_path, local_base_path)  # Preserve folder structure\n",
    "        object_name = relative_path.replace(\"\\\\\", \"/\")  # Ensure correct path format for MinIO\n",
    "\n",
    "        try:\n",
    "            client.fput_object(\n",
    "                bucket_name,  # Bucket name\n",
    "                object_name,  # Object path in MinIO\n",
    "                local_file_path,  # Local file path\n",
    "            )\n",
    "            print(f\"Uploaded: {local_file_path} -> {object_name}\")\n",
    "        except S3Error as e:\n",
    "            print(f\"Error uploading {local_file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8188bf9-f14e-4922-9472-88b6834893ff",
   "metadata": {},
   "source": [
    "## 6.1 LLM Workflow components\n",
    "\n",
    "Each component in the diagram corresponds to a [Cluster Workflow Templates](https://argo-workflows.readthedocs.io/en/latest/cluster-workflow-templates/) within Argo Workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e92aa5-7c71-41b4-9aa5-a80920ddcb5b",
   "metadata": {},
   "source": [
    "### 6.1.1 Creating Dataset Component (llm-workflows/components/1-create-dataset.yaml)\n",
    "\n",
    "This step involves invoking the Nemo Datastore API to create a dataset repository. Expected Behavior:\n",
    "\n",
    "- A request is sent to the Nemo Datastore API.\n",
    "- The dataset example-dataset is created in the default namespace.\n",
    "- The dataset gets a repository_id, which uniquely identifies it.\n",
    "\n",
    "**inputs**\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: ClusterWorkflowTemplate\n",
    "metadata:\n",
    "  name: nemo-create-dataset-template\n",
    "spec:\n",
    "  templates:\n",
    "    - name: create-dataset\n",
    "      inputs:\n",
    "        parameters:\n",
    "          - name: nemo_datastore_endpoint\n",
    "          - name: dataset_name\n",
    "          - name: namespace\n",
    "```\n",
    "**source code for creating dataset**\n",
    "\n",
    "```yaml\n",
    "            pip install huggingface_hub requests && \\\n",
    "            cat <<EOF > script.py\n",
    "            import requests\n",
    "            from huggingface_hub import HfApi\n",
    "            from huggingface_hub import configure_http_backend\n",
    "\n",
    "            def backend_factory():\n",
    "                session = requests.Session()\n",
    "                session.verify = False\n",
    "                return session\n",
    "\n",
    "            def create_repo(repo_id, repo_type):\n",
    "                hf_endpoint = \"{{inputs.parameters.nemo_datastore_endpoint}}/v1/hf\"\n",
    "                api = HfApi(endpoint=hf_endpoint, token=\"token\")\n",
    "                api.create_repo(repo_id=repo_id, repo_type=repo_type)\n",
    "\n",
    "            if __name__ == \"__main__\":\n",
    "                repo_id = \"{{inputs.parameters.namespace}}/{{inputs.parameters.dataset_name}}\"\n",
    "                repo_type = \"dataset\"\n",
    "                configure_http_backend(backend_factory=backend_factory)\n",
    "                create_repo(repo_id, repo_type)\n",
    "                f = open(\"/tmp/repo_id.txt\", \"w\")\n",
    "                f.write(repo_id)\n",
    "                f.close()\n",
    "\n",
    "```\n",
    "**output of the component**\n",
    "\n",
    "```yaml\n",
    "      outputs:\n",
    "        parameters:\n",
    "        - name: repo_id \n",
    "          valueFrom:\n",
    "            path: /tmp/repo_id.txt \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2308e0-f93b-4ad9-a5e0-155decf947dc",
   "metadata": {},
   "source": [
    "### 6.1.2 Upload Files to Dataset Component (llm-workflows/components/3-upload-files-dataset.yaml)\n",
    "\n",
    "This component involves uploading training, testing, and validation files into the Nemo Datastore. \n",
    "\n",
    "The training, testing, and validation datasets (typically in .jsonl format) are downloaded from the MinIO in a local directory and uploaded. \n",
    "\n",
    "**Inputs:**\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: ClusterWorkflowTemplate\n",
    "metadata:\n",
    "  name: nemo-upload-files-to-nemo-datastore-template\n",
    "spec:\n",
    "  templates:\n",
    "    - name: upload-files-to-nemo-datastore\n",
    "      inputs:\n",
    "        parameters:\n",
    "          - name: nemo_datastore_endpoint\n",
    "          - name: repo_id\n",
    "          - name: minio_url\n",
    "          - name: minio_username\n",
    "          - name: minio_password\n",
    "          - name: minio_bucket_name\n",
    "```\n",
    "\n",
    "\n",
    "**Downloading Dataset:**\n",
    "\n",
    "```yaml\n",
    "            def download_dataset():\n",
    "                # MinIO Configuration\n",
    "                minio_url = \"{{inputs.parameters.minio_url}}\"\n",
    "                access_key = \"{{inputs.parameters.minio_username}}\"\n",
    "                secret_key = \"{{inputs.parameters.minio_password}}\"\n",
    "                bucket_name = \"{{inputs.parameters.minio_bucket_name}}\"\n",
    "                local_download_path = \"/tmp\"  # Local base directory for downloads\n",
    "                \n",
    "                # Initialize MinIO Client\n",
    "                client = Minio(\n",
    "                    minio_url,\n",
    "                    access_key=access_key,\n",
    "                    secret_key=secret_key,\n",
    "                    secure=False,  # Set to True if using HTTPS\n",
    "                )\n",
    "                \n",
    "                # Ensure the local download directory exists\n",
    "                os.makedirs(local_download_path, exist_ok=True)\n",
    "                \n",
    "                # List and Download all objects\n",
    "                objects = client.list_objects(bucket_name, recursive=True)\n",
    "            \n",
    "                for obj in objects:\n",
    "                    object_name = obj.object_name  # Full path in MinIO\n",
    "                    local_file_path = os.path.join(local_download_path, object_name)\n",
    "            \n",
    "                    # Create directories if they don’t exist\n",
    "                    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "            \n",
    "                    # Download the file\n",
    "                    client.fget_object(bucket_name, object_name, local_file_path)\n",
    "                    print(f\"Downloaded: {object_name} -> {local_file_path}\")\n",
    "```\n",
    "\n",
    "**Uploading Dataset:** \n",
    "\n",
    "```yaml\n",
    "            def upload_datasets(repo_id, repo_type):\n",
    "                hf_endpoint = \"{{inputs.parameters.nemo_datastore_endpoint}}/v1/hf\"\n",
    "                hf_api = HfApi(endpoint=hf_endpoint, token=\"token\")\n",
    "                subprocess.run([\"find\", \"/tmp\"])\n",
    "\n",
    "                training_data_folder = \"/tmp/training\"  # Path to the folder\n",
    "                testing_data_folder = \"/tmp/testing\"  # Path to the folder\n",
    "                validation_data_folder = \"/tmp/validation\"  # Path to the folder\n",
    "\n",
    "                # Upload the folder\n",
    "                hf_api.upload_folder(\n",
    "                    folder_path=training_data_folder,\n",
    "                    repo_id=repo_id,\n",
    "                    repo_type=repo_type,\n",
    "                    path_in_repo=\"training\"\n",
    "                )\n",
    "\n",
    "                hf_api.upload_folder(\n",
    "                    folder_path=testing_data_folder,\n",
    "                    repo_id=repo_id,\n",
    "                    repo_type=repo_type,\n",
    "                    path_in_repo=\"testing\"\n",
    "                )\n",
    "\n",
    "                commit_info = hf_api.upload_folder(\n",
    "                    folder_path=validation_data_folder,\n",
    "                    repo_id=repo_id,\n",
    "                    repo_type=repo_type,\n",
    "                    path_in_repo=\"validation\"\n",
    "                )\n",
    "                print(commit_info)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58215ea-a3a2-40fb-850a-c85223e75806",
   "metadata": {},
   "source": [
    "### 6.1.3 Other components\n",
    "We have similar all other components in `llm-workflows/components`\n",
    "\n",
    "You can check them out individually. \n",
    "\n",
    "Example output: \n",
    "\n",
    "```\n",
    "llm-workflows/components/\n",
    "├── 1-create-dataset.yaml\n",
    "├── 2-print-repo-id.yaml\n",
    "├── 3-upload-files-dataset.yaml\n",
    "├── 4-register-dataset.yaml\n",
    "├── 5-create-customization.yaml\n",
    "├── 6-track-customization.yaml\n",
    "├── 7-create-evaluation-target.yaml\n",
    "├── 8-create-evaluation-config.yaml\n",
    "└── 9-create-evaluation.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a6a86-6886-4179-8d0d-dc7a0ffc1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree llm-workflows/components/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb5eec-831c-44cf-a96f-cf075b1f9c7d",
   "metadata": {},
   "source": [
    "## 6.2 Workflow Template (llm-workflows/workflow-templates/workflow-fine-tuning.yaml)\n",
    "The individual components are combined to form  [Workflow template](https://argo-workflows.readthedocs.io/en/latest/workflow-templates/), ensuring a modular and reusable structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085c915-4722-4157-937d-12b657030fd0",
   "metadata": {},
   "source": [
    "### 6.2.1 Inputs to the workflow\n",
    "\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: WorkflowTemplate\n",
    "metadata:\n",
    "  name: workflow-fine-tuning\n",
    "spec:\n",
    "  entrypoint: nemo-create-dataset\n",
    "  arguments:                    # You can pass in arguments as normal\n",
    "    parameters:\n",
    "      - name: nemo_datastore_endpoint\n",
    "        value: \"http://nemo-datastore.nemo-datastore.svc.cluster.local:3000\"\n",
    "      - name: nemo_customizer_endpoint\n",
    "        value: \"http://nemo-customizer-api.nemo-customizer.svc.cluster.local:8000\"\n",
    "      - name: nemo_evaluator_endpoint\n",
    "        value: \"http://nemo-evaluator.nemo-evaluator.svc.cluster.local:7331\"\n",
    "      - name: nemo_entity_store_endpoint\n",
    "        value: \"http://nemo-entity-store.nemo-entity-store.svc.cluster.local:8000\"\n",
    "      - name: nim_internal_endpoint\n",
    "        value: \"http://meta-llama3-1-8b-instruct.llama3-1-8b-instruct.svc.cluster.local:8000\"\n",
    "      - name: dataset_name_prefix\n",
    "        value: \"test\"\n",
    "      - name: namespace\n",
    "        value: \"default\"\n",
    "      - name: new_model_name\n",
    "        value: \"example-model@v3\"\n",
    "      - name: project_name\n",
    "        value: \"example-project-workflow\"\n",
    "      - name: minio_url\n",
    "        value: \"minio-client.minio.svc.cluster.local:9000\"\n",
    "      - name: minio_username\n",
    "        value: \"admin\"\n",
    "      - name: minio_password\n",
    "        value: \"QuX2+P16SPc7\"\n",
    "      - name: minio_bucket_name\n",
    "        value: \"test-dataset\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faff56a-7bba-43d4-9989-b7c6f923ef53",
   "metadata": {},
   "source": [
    "### 6.2.2 Inputs to the workflow and steps in the workflow\n",
    "\n",
    "```yaml\n",
    "\n",
    "    - name: nemo-create-dataset\n",
    "      steps:                              \n",
    "        - - name: create-dataset\n",
    "            templateRef:                  \n",
    "              name: nemo-create-dataset-template\n",
    "              template: create-dataset\n",
    "              clusterScope: true\n",
    "            arguments:                    \n",
    "              parameters:\n",
    "              - name: nemo_datastore_endpoint\n",
    "                value: \"{{workflow.parameters.nemo_datastore_endpoint}}\"\n",
    "              - name: dataset_name\n",
    "                value: \"{{workflow.parameters.dataset_name_prefix}}-{{workflow.name}}\"\n",
    "              - name: namespace\n",
    "                value: \"{{workflow.parameters.namespace}}\"\n",
    "[...]\n",
    "\n",
    "        - - name: nemo-customization\n",
    "            templateRef:                  \n",
    "              name: nemo-customization-template\n",
    "              template: nemo-customization\n",
    "              clusterScope: true\n",
    "            arguments:\n",
    "              parameters:    \n",
    "              - name: nemo_customizer_endpoint\n",
    "                value: \"{{workflow.parameters.nemo_customizer_endpoint}}\"\n",
    "              - name: dataset_name\n",
    "                value: \"{{workflow.parameters.dataset_name_prefix}}-{{workflow.name}}\"\n",
    "              - name: namespace\n",
    "                value: \"{{workflow.parameters.namespace}}\"\n",
    "              - name: project_name\n",
    "                value: \"{{workflow.parameters.project_name}}\"\n",
    "              - name: new_model_name\n",
    "                value: \"{{workflow.parameters.new_model_name}}\"\n",
    "\n",
    "[...]\n",
    "\n",
    "        - - name: create-eval-target\n",
    "            templateRef:                  \n",
    "              name: nemo-create-eval-target-template\n",
    "              template: create-eval-target\n",
    "              clusterScope: true\n",
    "            arguments:\n",
    "              parameters:\n",
    "              - name: nemo_evaluator_endpoint\n",
    "                value: \"{{workflow.parameters.nemo_evaluator_endpoint}}\"\n",
    "              - name: new_model_name\n",
    "                value: \"{{workflow.parameters.new_model_name}}\"\n",
    "              - name: nim_internal_endpoint\n",
    "                value: \"{{workflow.parameters.nim_internal_endpoint}}\"\n",
    "          - name: create-eval-config\n",
    "            templateRef:                  \n",
    "              name: nemo-create-eval-config-template\n",
    "              template: create-eval-config\n",
    "              clusterScope: true\n",
    "            arguments:\n",
    "              parameters:\n",
    "              - name: nemo_evaluator_endpoint\n",
    "                value: \"{{workflow.parameters.nemo_evaluator_endpoint}}\"\n",
    "              - name: namespace\n",
    "                value: \"{{workflow.parameters.namespace}}\"\n",
    "              - name: dataset_name\n",
    "                value: \"{{workflow.parameters.dataset_name_prefix}}-{{workflow.name}}\"           \n",
    "        - - name: create-evaluation\n",
    "            templateRef:                  \n",
    "              name: nemo-create-evaluation-template\n",
    "              template: create-evaluation\n",
    "              clusterScope: true\n",
    "            arguments:\n",
    "              parameters:\n",
    "              - name: nemo_evaluator_endpoint\n",
    "                value: \"{{workflow.parameters.nemo_evaluator_endpoint}}\"\n",
    "              - name: eval_target\n",
    "                value: \"{{steps.create-eval-target.outputs.parameters.eval_target}}\"\n",
    "              - name: eval_config\n",
    "                value: \"{{steps.create-eval-config.outputs.parameters.eval_config}}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6906cf8-2ff1-4feb-9948-0a23b1bf371b",
   "metadata": {},
   "source": [
    "## 6.3 Add llm-workflows to Git repo\n",
    "\n",
    "We will deploy the workflow components via ArgoCD into the argo workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f10c2-6d1f-43cf-be84-059757500ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove .ipynb_checkpoints if exists\n",
    "! rm -r llm-workflows/components/.ipynb_checkpoints/\n",
    "! rm -r llm-workflows/workflow-templates/.ipynb_checkpoints/\n",
    "! rm -r llm-workflows/workflow-templates/common/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69361daf-f354-46cc-a244-e6947888536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy llm-workflows to llmops-nvidia/\n",
    "!cp -r llm-workflows llmops-nvidia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d1189-68dd-4404-9d5c-8b6264625ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## See the directory structure\n",
    "!tree llmops-nvidia/llm-workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af021c-7c17-4b5c-a96c-0c47a6a77c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commit\n",
    "!git config --global user.email $commit_email\n",
    "!git config --global user.name $commit_user\n",
    "!cd llmops-nvidia/ && git add . && git commit -m \"add llm workflows\" && git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c043548-e43a-4740-81eb-54e6006986c8",
   "metadata": {},
   "source": [
    "## 6.4 Create application to track llm-workflows \n",
    "We will create an argocd application which will track all `llm-workflows`  under `llm-workflows` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463918a-6179-4b13-992b-c8814d754e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p llmops-nvidia/applications/llm-workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff3412-d249-4d10-9242-c8de78a99d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "argocd_workflows_application_yaml = f\"\"\"\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Application\n",
    "metadata:\n",
    "  name: llm-workflows\n",
    "  namespace: argocd\n",
    "spec:\n",
    "  destination:\n",
    "    namespace: nemo-workflow-templates\n",
    "    server: 'https://kubernetes.default.svc'\n",
    "  source:\n",
    "    path: llm-workflows\n",
    "    repoURL: '{git_repo_url_ssh}'\n",
    "    targetRevision: main\n",
    "    directory:\n",
    "      recurse: true\n",
    "  project: default\n",
    "  syncPolicy:\n",
    "    syncOptions:\n",
    "    - Validate=false\n",
    "    - CreateNamespace=true\n",
    "    automated:\n",
    "      prune: true\n",
    "      selfHeal: true\n",
    "      allowEmpty: false\n",
    "\"\"\"\n",
    "with open(\"llmops-nvidia/applications/llm-workflows/app.yaml\", \"w\") as f:\n",
    "    f.write(argocd_workflows_application_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29f6bd-8322-4011-997c-ab8e3b9d8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email $commit_email\n",
    "!git config --global user.name $commit_user\n",
    "!cd llmops-nvidia/ && git add . && git commit -m \"add llm workflows tracking app\" && git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7174337-1b86-4d10-bc82-ba9fced12a3d",
   "metadata": {},
   "source": [
    "### 6.4.1 Sync vi UI\n",
    "As we have commit our code to Git, argocd usually sync automatically after every 5 minutes. But we can force it to sync either via UI or CLI. \n",
    "\n",
    "Here we use UI to sync. \n",
    "\n",
    "Click on the sycn button on the `argocd-components` application as shown in diagram, then the `llm-workflows` would show up. \n",
    "<img src=\"./images-dli/sync-apps.png\" style=\"width: 435px; float: left\">\n",
    "<img src=\"./images-dli/llm-workflows-app.png\" style=\"width: 500px; float: right\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5be005-e811-44eb-aa3a-5ac96d91e6e7",
   "metadata": {},
   "source": [
    "## 6.5 Check llm-workflows components in Argo Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bdefdf-84a6-4921-a192-642b550ec4c1",
   "metadata": {},
   "source": [
    "### 6.5.1 Port forward Argo workflows for accessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557ad4f-6eb0-4419-8020-92bb6a0fcc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.Popen(\n",
    "    [\"kubectl\", \"-n\", \"argoworkflows\", \"port-forward\", \"--address\", \"0.0.0.0\", \"service/argo-workflows-server\", \"31091:2746\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    "    close_fds=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54035be0-8658-4e7e-b72e-5eaa8e835bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname;\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Argoworkflow UI!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href + \"/\";\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791cf81-72ed-43b3-8486-550b1e34dc1a",
   "metadata": {},
   "source": [
    "### 6.5.2 LLM Workflow Components\n",
    "\n",
    "Once you open the Argo Workflows UI: \n",
    "1. Click on the `Cluster Workflow Templates` option from the side menu.\n",
    "2. You can check the components byt clicking on them. \n",
    "    \n",
    "<center><img src=\"./images-dli/workflow-components.png\" style=\"width: 650px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c998ca-9102-4969-96ee-0de1fc6a2bf0",
   "metadata": {},
   "source": [
    "### 6.5.2 LLM Workflow Templates\n",
    "The individual components are combined to form  [Workflow template](https://argo-workflows.readthedocs.io/en/latest/workflow-templates/), ensuring a modular and reusable structure.\n",
    "\n",
    "Once you open the Argo Workflows UI: \n",
    "1. Click on the `Workflow Templates` option from the side menu and select `workflow-fine-tuning`\n",
    "\n",
    "<center><img src=\"./images-dli/workflow-templates-submit.png\" style=\"width: 650px;\"></center>\n",
    "\n",
    "2. The workflow will show up.\n",
    "\n",
    "<center><img src=\"./images-dli/workflow-template-yaml.png\" style=\"width: 650px;\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742222c5-8340-4faa-89df-6c8b2212cff4",
   "metadata": {},
   "source": [
    "### 6.5.3 LLM Workflow Submission\n",
    "\n",
    "1. Select the workflow template and click on `Submit` on the top left\n",
    "    \n",
    "<center><img src=\"./images-dli/workflow-templates-submit.png\" style=\"width: 650px;\"></center>\n",
    "\n",
    "2. You can edit the variables and click submit. The automation pipeline of the E2E workflow will run. \n",
    "\n",
    "<center><img src=\"./images-dli/workflow-submission.png\" style=\"width: 650px;\"></center>\n",
    "\n",
    "3. At the end of the fine-tuning workflow, evaluation workflow will also be launched.\n",
    "\n",
    "\n",
    "<center><img src=\"./images-dli/eval-workflow-fine-tuning.png\" style=\"width: 650px;\"></center>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae6dd8-c398-4b11-b887-3a40d5c92623",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The overall workflow takes around 10-12 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7522bd-d066-410c-9f2e-e15aacf63428",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've made it through the last Notebook. In this notebook, you have:\n",
    "- Explored various argo workflows components created which reporesents the Cluster Worklfow tempaltes in argo workflows.\n",
    "- Explored Workflow Templates in Argo Workflows.\n",
    "- Synced all workflow components and templates via ArgoCD.\n",
    "- Submit the workflow to complete E2E automation of fine-tuning pipeline.\n",
    "- Monitor the status of the workflow process in Argo workflows UI. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
