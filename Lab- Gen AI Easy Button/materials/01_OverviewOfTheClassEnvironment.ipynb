{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.0 Overview of the Class Environment\n",
    "\n",
    "<img src=\"images/overview_class.png\" style=\"float: right;\" width=350>\n",
    "\n",
    "This notebook will introduce the class environment that was configured to mimic an AI production structure. You will have an overview of the Class environment configured as a Kubernetes cluster with 4x A100 80GB GPU resources.\n",
    "\n",
    "In addition, you will experiment with basic commands of the [Kubernetes Cluster](https://kubernetes.io/). Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. \n",
    "\n",
    "In this class, a K8s cluster is already launched using [Minikube](https://minikube.sigs.k8s.io/docs/). In addition, we already enabled the cluster for GPU acceleration using GPU Operator. \n",
    "\n",
    "This is our first step toward deploying, monitoring and managing AI based applications in production. \n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Understand the hardware configuration available for the class\n",
    "* Understand the basics Kubernetes commands \n",
    "* Run a simple Cuda application\n",
    "\n",
    "**[1.1 The Hardware Configuration Overview](#1.1-The-Hardware-Configuration-Overview)<br>**\n",
    "**[1.2 Kubernetes Cluster Basics](#1.2-Kubernetes-Cluster-Basics)<br>**\n",
    "**[1.3 GPU Application Example](#1.3-GPU-Application-Example)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.1 The Hardware Configuration Overview\n",
    "\n",
    "NVIDIA has designed DGXs servers as a full-stack solution for scalable AI development. Click the link to learn more about [DGX systems](https://www.nvidia.com/en-gb/data-center/dgx-systems/). This class environment is built with half the resources of a DGX 8xA100 server system (4x A100 GPUs, 4 NVlinks per GPU). However, different deliveries of this course may have different hardware configurations. Thus, for benchmarking purposes, we will be using 4x A100 80G as a reference.\n",
    "\n",
    "<img  src=\"images/nvlink_v2.png\" width=\"400\"/>\n",
    "\n",
    "The hardware setup for this course has been pre-configured as a GPU cluster unit. The cluster is structured into compute nodes (compute units), which can be managed by a Cluster Manager such as Kubernetes. Alongside CPUs (Central Processing Units) and GPUs (Graphics Processing Units), the cluster incorporates storage and networking components.\n",
    "\n",
    "Let's look at the Hardware design available in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Check The Available CPUs \n",
    "\n",
    "We can check the CPU information of the system using the `lscpu` command. \n",
    "This example of outputs shows that there are 48 CPU cores of the `x86_64` from AMD.\n",
    "```\n",
    "Architecture:                    x86_64\n",
    "Core(s) per socket:              48\n",
    "Model name:                      AMD EPYC 7V13 64-Core Processor\n",
    "```\n",
    "For a complete description of the CPU processor architecture, check the `/proc/cpuinfo` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display CPUs information\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the number of CPU cores\n",
    "!grep 'cpu cores' /proc/cpuinfo | uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Check The Available  GPUs \n",
    "\n",
    "The NVIDIA System Management Interface `nvidia-smi` is a command for monitoring NVIDIA GPU devices. Several key details are listed such as the CUDA and  GPU driver versions, the number and type of GPUs available, the GPU memory each, running GPU process, etc.\n",
    "\n",
    "In the following example, `nvidia-smi` command shows that there are GPUs, each with approximately 80GB of memory. \n",
    "\n",
    "<img  src=\"images/nvidia-smi.png\" width=\"600\"/>\n",
    "\n",
    "For more details, refer to the [nvidia-smi documentation](https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display information about GPUs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 Kubernetes Cluster Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, a local Kubernetes cluster is already running using [Minikube](https://minikube.sigs.k8s.io/docs/) and the NVIDIA GPU Operator is already installed.\n",
    "\n",
    "The [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html) uses the operator framework within Kubernetes to automate the management of all NVIDIA software components needed to provision GPU. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Toolkit, automatic node labelling using GPU Feautre Discovery (GFD), DCGM based monitoring and others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kubectl get` commands provides information about nodes, pods and services in your Kubernetes cluster. Learn more about [kubectl command line tool](https://kubernetes.io/docs/reference/kubectl/overview/).\n",
    "\n",
    "In a Kubernetes cluster, there are 2 types of ressources: Nodes running the applications and a control-plane coordinating the cluster. \n",
    "\n",
    "Let's have a look at the lab Kubernetes cluster by checking the status of nodes and Pods.\n",
    "\n",
    "`kubectl get nodes` fetches the status of all nodes in the Kubernetes cluster from the control plane node. \n",
    "\n",
    "In this lab, you should see the control-plane node called minikube with the status `Ready`. \n",
    "\n",
    "Let's check that by running the below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "If you get error when running the below command, it means the cluster is not ready. Try in few minutes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check available nodes in the cluster\n",
    "! kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your cluster is already running a minikube control-plane node.\n",
    "\n",
    "You can get more output details using the argument `-o wide`. Notice the Internal IP of minikube. \n",
    "\n",
    "Kubernetes requires a container runtime. In our class, we are using docker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check available nodes in the cluster with more details\n",
    "! kubectl get nodes -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the pods running on our Kubernetes cluster.\n",
    "You should see a several running pods on kube-system and gpu-operator namespace. Note, we are using -A to get pods in all namespaces. \n",
    "\n",
    "```\n",
    "NAMESPACE      NAME                                                              READY   STATUS      RESTARTS        AGE\n",
    "gpu-operator   gpu-feature-discovery-24m7m                                       1/1     Running     0               7m26s\n",
    "gpu-operator   gpu-operator-1708880160-node-feature-discovery-gc-7fd59b8cdnjxb   1/1     Running     0               8m1s\n",
    "gpu-operator   gpu-operator-1708880160-node-feature-discovery-master-55b7869lr   1/1     Running     0               8m1s\n",
    "gpu-operator   gpu-operator-1708880160-node-feature-discovery-worker-bxgzc       1/1     Running     0               8m1s\n",
    "gpu-operator   gpu-operator-fbc85568f-4l2kx                                      1/1     Running     1 (6m32s ago)   8m1s\n",
    "gpu-operator   nvidia-container-toolkit-daemonset-fb54t                          1/1     Running     0               7m23s\n",
    "gpu-operator   nvidia-cuda-validator-s8vh2                                       0/1     Completed   0               7m19s\n",
    "gpu-operator   nvidia-dcgm-exporter-v2b99                                        1/1     Running     0               7m26s\n",
    "gpu-operator   nvidia-device-plugin-daemonset-2wx9k                              1/1     Running     0               7m26s\n",
    "gpu-operator   nvidia-mig-manager-55lr2                                          1/1     Running     0               5m53s\n",
    "gpu-operator   nvidia-operator-validator-7d8hx                                   1/1     Running     0               7m26s\n",
    "kube-system    coredns-5dd5756b68-wvzkb                                          1/1     Running     0               8m1s\n",
    "kube-system    etcd-minikube                                                     1/1     Running     0               8m13s\n",
    "kube-system    kube-apiserver-minikube                                           1/1     Running     0               8m15s\n",
    "kube-system    kube-controller-manager-minikube                                  1/1     Running     0               8m13s\n",
    "kube-system    kube-proxy-qw8qm                                                  1/1     Running     0               8m1s\n",
    "kube-system    kube-scheduler-minikube                                           1/1     Running     0               8m15s\n",
    "kube-system    nvidia-device-plugin-daemonset-j2b69                              1/1     Running     0               8m1s\n",
    "kube-system    storage-provisioner                                               1/1     Running     1 (6m31s ago)   8m12s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the resources we have in the minikube cluster. Notice the available GPUs and CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl get node minikube -o jsonpath='{.status.capacity}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 GPU Application Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will deploy a simple GPU-accelerated application. \n",
    "\n",
    "We will use the [cuda-vectoradd](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#cuda-vectoradd) toy application which randomly generates two very large vectors and adds them. \n",
    "\n",
    "Let's print out the YAML configuration file needed to deploy this application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the cuda-vectoradd config \n",
    "!cat kubernetes-config/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config file shows a pod named `gpu-operator-test` deploying cuda-vector-add on nvidia/samples:vectoradd-cuda11.6.0 container using 1 GPU. \n",
    "\n",
    "To deploy an application, execute the `kubectl apply` command, specifying the YAML configuration file with the `-f` file option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy the application (run the pod)\n",
    "!kubectl apply -f kubernetes-config/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deployed, we can observe the status of a pod created with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the status of the pod deployed\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see the status Pending or ContainerCreating. Try again after few seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run again to Get the status of the pod Completed\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our application status is now `Completed`. Let's have a look at its execution logs with `kubectl logs`. \n",
    "\n",
    "You should see outputs similar to:\n",
    "```\n",
    "[Vector addition of 50000 elements]\n",
    "Copy input data from the host memory to the CUDA device\n",
    "CUDA kernel launch with 196 blocks of 256 threads\n",
    "Copy output data from the CUDA device to the host memory\n",
    "Test PASSED\n",
    "Done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the output\n",
    "!kubectl logs gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's delete the Kubernetes pod `gpu-operator-test` as we do not needed anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's delete the pod\n",
    "!kubectl delete -f kubernetes-config/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've made it through the first section. In this notebook, you have:\n",
    "- Discovered the class environment configuration.\n",
    "- Interacted with K8s using `kubectl`\n",
    "- Deployed a simple Cuda application\n",
    "\n",
    "Next, you'll see how to deploy NIMs forming a complex Retrieval Augmented Generation application (RAG).\n",
    "\n",
    "Move on to [02_GitOps_Using_ArgoCD.ipynb](02_GitOps_Using_ArgoCD.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
